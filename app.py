import streamlit as st
import os
import re
import openai
from sentence_transformers import SentenceTransformer
from pinecone import Pinecone
from rank_bm25 import BM25Okapi
import json

st.set_page_config(page_title="Univera RAG Chatbot", page_icon="ğŸ¤–", layout="wide")

@st.cache_resource
def initialize_models():
    """ëª¨ë¸ ë° API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    # OpenAI ì„¤ì •
    openai_api_key = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.stop()
    
    openai_client = openai.OpenAI(api_key=openai_api_key)
    
    # Pinecone ì„¤ì •
    pinecone_api_key = st.secrets.get("PINECONE_API_KEY") or os.getenv("PINECONE_API_KEY")
    pinecone_index_name = st.secrets.get("PINECONE_INDEX_NAME") or os.getenv("PINECONE_INDEX_NAME")
    
    if not pinecone_api_key or not pinecone_index_name:
        st.error("Pinecone API í‚¤ ë˜ëŠ” ì¸ë±ìŠ¤ ì´ë¦„ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.stop()
    
    pc = Pinecone(api_key=pinecone_api_key)
    pinecone_index = pc.Index(pinecone_index_name)
    
    # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    embedding_model = SentenceTransformer("intfloat/multilingual-e5-base")
    
    return openai_client, pinecone_index, embedding_model

class UniveraRAG:
    def __init__(self, openai_client, pinecone_index, embedding_model):
        self.openai_client = openai_client
        self.pinecone_index = pinecone_index
        self.model = embedding_model
        self.gpt_model = "gpt-4o-mini"
        
        # BM25ë¥¼ ìœ„í•œ ë¬¸ì„œ ìºì‹œ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë³„ë„ ì €ì¥ì†Œ ì‚¬ìš© ê¶Œì¥)
        if 'bm25_corpus' not in st.session_state:
            st.session_state.bm25_corpus = []
            st.session_state.bm25_model = None
    
    def tokenize(self, text):
        """í…ìŠ¤íŠ¸ í† í°í™”"""
        if not text or not isinstance(text, str):
            return []
        
        # í•œê¸€, ì˜ì–´, ìˆ«ì ì¶”ì¶œ
        tokens = re.findall(r'[ê°€-í£a-zA-Z0-9]+', text.lower())
        return [token for token in tokens if len(token) >= 2]
    
    def embed(self, text, is_query=False):
        """í…ìŠ¤íŠ¸ ì„ë² ë”©"""
        if not text:
            return []
        
        prefix = "query: " if is_query else "passage: "
        return self.model.encode(prefix + text, normalize_embeddings=True)
    
    def vector_search(self, query, top_k=15):
        """ë²¡í„° ê²€ìƒ‰"""
        query_vec = self.embed(query, is_query=True)
        
        try:
            results = self.pinecone_index.query(
                vector=query_vec.tolist(),
                top_k=top_k,
                include_metadata=True
            )
            
            vector_results = []
            for match in results.matches:
                if 'text' in match.metadata:
                    vector_results.append({
                        'text': match.metadata['text'],
                        'score': float(match.score),
                        'source': match.metadata.get('source', 'Unknown'),
                        'type': 'vector'
                    })
            
            return vector_results
        except Exception as e:
            st.error(f"ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def bm25_search(self, query, top_k=10):
        """BM25 ê²€ìƒ‰ (ì œí•œì  êµ¬í˜„)"""
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì‚¬ì „ì— êµ¬ì¶•ëœ BM25 ì¸ë±ìŠ¤ ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë§¤ì¹­ ìˆ˜í–‰
        if not st.session_state.bm25_corpus:
            return []
        
        try:
            tokenized_query = self.tokenize(query)
            if not tokenized_query:
                return []
            
            if st.session_state.bm25_model is None:
                return []
            
            scores = st.session_state.bm25_model.get_scores(tokenized_query)
            top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]
            
            bm25_results = []
            for idx in top_indices:
                if scores[idx] > 0:
                    bm25_results.append({
                        'text': st.session_state.bm25_corpus[idx]['text'],
                        'score': float(scores[idx]),
                        'source': st.session_state.bm25_corpus[idx].get('source', 'Unknown'),
                        'type': 'bm25'
                    })
            
            return bm25_results
        except Exception as e:
            st.warning(f"BM25 ê²€ìƒ‰ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
            return []
    
    def normalize_scores(self, results):
        """ì ìˆ˜ ì •ê·œí™”"""
        if not results:
            return results
        
        scores = [r['score'] for r in results]
        if not scores:
            return results
        
        min_score = min(scores)
        max_score = max(scores)
        
        if max_score == min_score:
            for result in results:
                result['normalized_score'] = 1.0
        else:
            for result in results:
                result['normalized_score'] = (result['score'] - min_score) / (max_score - min_score)
        
        return results
    
    def hybrid_search(self, query, vector_weight=0.7, bm25_weight=0.3, final_top_k=5):
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        # ë²¡í„° ê²€ìƒ‰
        vector_results = self.vector_search(query, top_k=15)
        
        # BM25 ê²€ìƒ‰
        bm25_results = self.bm25_search(query, top_k=10)
        
        # ì ìˆ˜ ì •ê·œí™”
        vector_results = self.normalize_scores(vector_results)
        bm25_results = self.normalize_scores(bm25_results)
        
        # ê²°ê³¼ í†µí•©
        combined_results = {}
        
        # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
        for result in vector_results:
            text = result['text']
            score = result['normalized_score'] * vector_weight
            combined_results[text] = {
                'text': text,
                'score': score,
                'source': result['source'],
                'types': ['vector']
            }
        
        # BM25 ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€/ì—…ë°ì´íŠ¸
        for result in bm25_results:
            text = result['text']
            score = result['normalized_score'] * bm25_weight
            
            if text in combined_results:
                combined_results[text]['score'] += score
                combined_results[text]['types'].append('bm25')
            else:
                combined_results[text] = {
                    'text': text,
                    'score': score,
                    'source': result['source'],
                    'types': ['bm25']
                }
        
        # ìµœì¢… ê²°ê³¼ ì •ë ¬ ë° ìƒìœ„ kê°œ ì„ íƒ
        final_results = sorted(combined_results.values(), key=lambda x: x['score'], reverse=True)[:final_top_k]
        
        return final_results, len(vector_results), len(bm25_results)
    
    def create_context(self, search_results):
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        context_parts = []
        for i, result in enumerate(search_results, 1):
            source = result.get('source', 'Unknown')
            text = result['text']
            context_parts.append(f"[ë¬¸ì„œ {i}] ({source}):\n{text}")
        
        return "\n\n".join(context_parts)
    
    def generate_answer(self, query, search_results, max_tokens=1000):
        """GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±"""
        if not search_results:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ì„œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []
        
        context = self.create_context(search_results)
        
        system_prompt = """ë‹¹ì‹ ì€ Univera íšŒì‚¬ ì •ë³´ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë‹µë³€ ê·œì¹™:
1. ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
2. ì •í™•í•˜ì§€ ì•Šê±°ë‚˜ í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì œê³µí•˜ì§€ ë§ˆì„¸ìš”
3. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
4. ê°€ëŠ¥í•˜ë©´ êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ì„¸ë¶€ì‚¬í•­ì„ í¬í•¨í•˜ì„¸ìš”
5. ì»¨í…ìŠ¤íŠ¸ì— ê´€ë ¨ ì •ë³´ê°€ ì—†ë‹¤ë©´ ì†”ì§íˆ "í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•˜ì„¸ìš”

ë‹µë³€ í˜•ì‹:
- ëª…í™•í•˜ê³  êµ¬ì¡°í™”ëœ ë‹µë³€
- í•„ìš”ì‹œ ë¶ˆë¦¿ í¬ì¸íŠ¸ ì‚¬ìš©
- ì¶œì²˜ ì •ë³´ í¬í•¨ (ì˜ˆ: "ë¬¸ì„œì— ë”°ë¥´ë©´...")
"""

        user_prompt = f"""ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {query}

ìœ„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."""

        try:
            response = self.openai_client.chat.completions.create(
                model=self.gpt_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=max_tokens,
                temperature=0.1
            )
            
            answer = response.choices[0].message.content
            return answer, search_results
            
        except Exception as e:
            error_msg = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            return error_msg, search_results

def main():
    st.title("ğŸ¤– Univera RAG Chatbot")
    st.markdown("---")
    
    # ì´ˆê¸°í™”
    openai_client, pinecone_index, embedding_model = initialize_models()
    rag_system = UniveraRAG(openai_client, pinecone_index, embedding_model)
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ ê²€ìƒ‰ ì„¤ì •")
        
        vector_weight = st.slider("ë²¡í„° ê²€ìƒ‰ ê°€ì¤‘ì¹˜", 0.0, 1.0, 0.7, 0.1)
        bm25_weight = st.slider("BM25 ê²€ìƒ‰ ê°€ì¤‘ì¹˜", 0.0, 1.0, 0.3, 0.1)
        final_top_k = st.slider("ìµœì¢… ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜", 1, 10, 5)
        max_tokens = st.slider("ìµœëŒ€ ë‹µë³€ í† í°", 500, 2000, 1000, 100)
        
        # ê°€ì¤‘ì¹˜ í•©ì´ 1ì´ ë˜ë„ë¡ ì¡°ì •
        total_weight = vector_weight + bm25_weight
        if total_weight != 1.0:
            st.warning("ê°€ì¤‘ì¹˜ í•©ì´ 1.0ì´ ë˜ë„ë¡ ìë™ ì¡°ì •ë©ë‹ˆë‹¤.")
            vector_weight = vector_weight / total_weight
            bm25_weight = bm25_weight / total_weight
        
        st.markdown("---")
        st.markdown("### ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ")
        st.success("âœ… OpenAI ì—°ê²°ë¨")
        st.success("âœ… Pinecone ì—°ê²°ë¨") 
        st.success("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œë¨")
    
    # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! Univera ê´€ë ¨ ì§ˆë¬¸ì„ ììœ ë¡­ê²Œ ë¬¼ì–´ë³´ì„¸ìš”. ğŸ˜Š"}
        ]
    
    # ì´ì „ ëŒ€í™” í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš° í‘œì‹œ
            if "search_results" in message:
                with st.expander("ğŸ” ì°¸ê³ ëœ ë¬¸ì„œ"):
                    for i, result in enumerate(message["search_results"], 1):
                        st.markdown(f"**ë¬¸ì„œ {i}** (ìŠ¤ì½”ì–´: {result['score']:.3f})")
                        st.markdown(f"**ì¶œì²˜:** {result['source']}")
                        st.markdown(f"**ê²€ìƒ‰ ë°©ë²•:** {', '.join(result.get('types', ['vector']))}")
                        st.markdown(f"**ë‚´ìš©:** {result['text'][:200]}...")
                        st.markdown("---")
    
    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("Univeraì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # AI ë‹µë³€ ìƒì„±
        with st.chat_message("assistant"):
            with st.spinner("ê²€ìƒ‰ ì¤‘..."):
                # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
                search_results, vector_count, bm25_count = rag_system.hybrid_search(
                    prompt, vector_weight, bm25_weight, final_top_k
                )
                
                # ê²€ìƒ‰ í†µê³„ í‘œì‹œ
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ë²¡í„° ê²€ìƒ‰", vector_count)
                with col2:
                    st.metric("BM25 ê²€ìƒ‰", bm25_count)
                with col3:
                    st.metric("ìµœì¢… ê²°ê³¼", len(search_results))
            
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                # ë‹µë³€ ìƒì„±
                answer, used_results = rag_system.generate_answer(prompt, search_results, max_tokens)
                
                # ë‹µë³€ í‘œì‹œ
                st.markdown(answer)
                
                # ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
                if used_results:
                    with st.expander("ğŸ” ì°¸ê³ ëœ ë¬¸ì„œ"):
                        for i, result in enumerate(used_results, 1):
                            st.markdown(f"**ë¬¸ì„œ {i}** (ìŠ¤ì½”ì–´: {result['score']:.3f})")
                            st.markdown(f"**ì¶œì²˜:** {result['source']}")
                            st.markdown(f"**ê²€ìƒ‰ ë°©ë²•:** {', '.join(result.get('types', ['vector']))}")
                            st.markdown(f"**ë‚´ìš©:** {result['text'][:200]}...")
                            st.markdown("---")
        
        # ë‹µë³€ì„ ì„¸ì…˜ì— ì €ì¥
        st.session_state.messages.append({
            "role": "assistant", 
            "content": answer,
            "search_results": used_results
        })
    
    # í•˜ë‹¨ ì •ë³´
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: #666;'>
        <small>Univera RAG Chatbot â€¢ Powered by OpenAI GPT-4o-mini & Pinecone â€¢ ë‹¤êµ­ì–´ ì„ë² ë”© ì§€ì›</small>
        </div>
        """,
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()